{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eraneo/Projek-Akhir-Pemrosesan-Teks/blob/main/Salinan_Source_Code_Progess_2_Projek_Pemrosesan_Teks_Kelompok_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Projek Akhir Pemrosesan Teks**\n",
        "\n",
        "Dosen pengampu: Ulfa Siti Nuraini, S.Stat., M.Stat.\n",
        "\n",
        "Kelompok: 8\n",
        "\n",
        "1. Elvian Eraneo Subroto (251155001)\n",
        "2. Na'illah Shania Rizqillah (24031554225)\n",
        "3. Refochella Florentine Gwacevo Pertiwi (24031554045)"
      ],
      "metadata": {
        "id": "WVE4bpTC8GCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Pengambilan data**"
      ],
      "metadata": {
        "id": "3MubvCOpWh1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data komen tiktok"
      ],
      "metadata": {
        "id": "MI92RJZRDjKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"! pip install apify_client pandas\"\"\""
      ],
      "metadata": {
        "id": "-HQoShslD4Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from apify_client import ApifyClient\n",
        "\n",
        "# Initialize the ApifyClient with your API token\n",
        "client = ApifyClient(\"apify_api_VZjgYMcz4sHOdg8T829UZZ0WYdCsXG3WlT2s\")\n",
        "\n",
        "# Prepare the Actor input\n",
        "run_input = {\n",
        "    \"commentsPerPost\": 500,\n",
        "    \"excludePinnedPosts\": False,\n",
        "    \"maxRepliesPerComment\": 50,\n",
        "    \"postURLs\": [\n",
        "        \"https://www.tiktok.com/@sellersolution/video/7573169400082156808?q=tanggapan%20terkait%20jualan%20online%3F&t=1763721521730\",\n",
        "        \"https://www.tiktok.com/@aziz.yevo/video/7563892053151714578?q=tanggapan%20terkait%20jualan%20online%3F&t=1763721521730\",\n",
        "        \"https://www.tiktok.com/@bobaacaramel507/video/7528070975204625682?q=keluh%20kesah%20belanja%20online%3F&t=1763721748467\",\n",
        "        \"https://www.tiktok.com/@opangverse/video/7549686218196258104?q=keluh%20kesah%20belanja%20online%3F&t=1763721748467\"\n",
        "    ],\n",
        "    \"resultsPerPage\": 100\n",
        "}\n",
        "\n",
        "# Run the Actor and wait for it to finish\n",
        "run = client.actor(\"BDec00yAmCm1QbMEI\").call(run_input=run_input)\n",
        "\n",
        "items = list(client.dataset(run[\"defaultDatasetId\"]).iterate_items())\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "ueA0ahN8DlzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"df_tiktok = pd.DataFrame(items)\n",
        "df_tiktok.to_excel(\"komentar tiktok.xlsx\")\"\"\""
      ],
      "metadata": {
        "id": "nc5qosbwLTXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Preprocessing data komentar tiktok**"
      ],
      "metadata": {
        "id": "0f3bUOg0eKKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MZBQmPQLKK-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tiktok = pd.read_excel(\"/content/komentar tiktok.xlsx\")"
      ],
      "metadata": {
        "id": "EsD9oK0neHxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install demoji"
      ],
      "metadata": {
        "id": "2aTKeE2aRIE7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import demoji\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qZVnEEVwRIm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercasing\n",
        "tiktok_prepro1 = []\n",
        "for i in data_tiktok[\"text\"]:\n",
        "  hasil_cf = str(i).lower()\n",
        "  tiktok_prepro1.append(hasil_cf)\n",
        "\n",
        "# remove html tags\n",
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+\\n', '', text)\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "def remove_mentions(text):\n",
        "    return re.sub(r'@[A-Za-z0-9_.]+', '', text)\n",
        "\n",
        "tiktok_prepro2 = []\n",
        "for i in tiktok_prepro1:\n",
        "  hasil_cf1 = remove_mentions(i)\n",
        "  hasil_cf = remove_html(hasil_cf1)\n",
        "  tiktok_prepro2.append(hasil_cf)\n",
        "\n",
        "# remove punctuation\n",
        "punc = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', punc))\n",
        "\n",
        "tiktok_prepro3 = []\n",
        "for i in tiktok_prepro2:\n",
        "  hasil_cf = remove_punctuation(i)\n",
        "  tiktok_prepro3.append(hasil_cf)\n",
        "\n",
        "# remove emoji\n",
        "def remove_emoji(text):\n",
        "    cleaned_text = demoji.replace(text,repl=\"\") # Replaces emojis with an empty string\n",
        "    #cleaned_text = demoji.replace_with_desc(text) # Replaces emojis with text\n",
        "    return cleaned_text\n",
        "\n",
        "tiktok_prepro4 = []\n",
        "for i in tiktok_prepro3:\n",
        "  hasil_cf = remove_emoji(i)\n",
        "  tiktok_prepro4.append(hasil_cf)"
      ],
      "metadata": {
        "id": "RH0-j4R-epk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiktok_sudah_diprepro = pd.DataFrame({'Sebelum Preprocessing': data_tiktok[\"text\"],'Setelah dipreprocessing':tiktok_prepro4})"
      ],
      "metadata": {
        "id": "NfTG-d5xfB6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filtering data**"
      ],
      "metadata": {
        "id": "F_hu3hIo6uFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tiktok_sudah_diprepro['Setelah dipreprocessing'] = tiktok_sudah_diprepro['Setelah dipreprocessing'].replace(\"\", np.nan)"
      ],
      "metadata": {
        "id": "UAhvdl-ZhbYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tiktok_sudah_diprepro.dropna(subset = [\"Sebelum Preprocessing\",\"Setelah dipreprocessing\"])"
      ],
      "metadata": {
        "id": "4oRRSUs7N5e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bersih = pd.DataFrame({\n",
        "    'text': x[\"Setelah dipreprocessing\"],\n",
        "    'label': np.nan\n",
        "})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RgqXftlEOBUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bersih.to_excel(\"komentar tiktok (sudah bersih).xlsx\")"
      ],
      "metadata": {
        "id": "mdso7cd8WtWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bersih.info()"
      ],
      "metadata": {
        "id": "N1Z2rt8DAj09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Labeling manual**"
      ],
      "metadata": {
        "id": "XCyDq2su1y6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# ambil 10% data untuk labeling manual\n",
        "df_labeling = df_bersih.sample(frac=0.10, random_state=42)\n",
        "\n",
        "df_labeling.to_excel(\"komentar_untuk_dilabel.xlsx\", index=False)\"\"\""
      ],
      "metadata": {
        "id": "IGKgkKis11J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. split data**"
      ],
      "metadata": {
        "id": "8lx7y17dlalu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/komentar_untuk_dilabel.csv\", encoding='latin-1', delimiter=';')\n",
        "\n",
        "# Define label mappings for the model's internal use (0-indexed)\n",
        "label2id = {\"negatif\":0, \"netral\":1, \"positif\":2}\n",
        "id2label = {0:\"negatif\", 1:\"netral\", 2:\"positif\"}"
      ],
      "metadata": {
        "id": "ZJIUkUxVrE3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "RHpv-0oZh1Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map numerical labels from -1, 0, 1 to 0, 1, 2\n",
        "# Assuming -1 is negatif, 0 is netral, 1 is positif\n",
        "label_remap = {-1: 0, 0: 1, 1: 2}\n",
        "df[\"label\"] = df[\"label\"].map(label_remap)"
      ],
      "metadata": {
        "id": "27-acNO1tfjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "dmAYS92VjNcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.utils import stratify\n",
        "\"split dilakukan sekali saja, jangan di run lagi\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# stratified split 80:20\n",
        "teks_train, teks_test, label_train, label_test = train_test_split(\n",
        "    df[\"text\"], df[\"label\"],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(teks_train))\n",
        "print(\"Test size :\", len(teks_test))\n",
        "\n",
        "# gabung lagi jadi dataframe kalau mau\n",
        "train_df = pd.DataFrame({\"text\": teks_train, \"label\": label_train})\n",
        "test_df  = pd.DataFrame({\"text\": teks_test, \"label\": label_test})"
      ],
      "metadata": {
        "id": "qCKzw2eiwFHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jadiin csv\n",
        "train_df.to_csv(\"train.csv\", index=False)\n",
        "test_df.to_csv(\"test.csv\", index=False)"
      ],
      "metadata": {
        "id": "9CejREevAv15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Pemodelan dengan indobert**"
      ],
      "metadata": {
        "id": "ClMifGfHqk8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate torch accelerate --quiet"
      ],
      "metadata": {
        "id": "un1lMPsYqnJX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "q40cd9s2rAmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/train.csv\", encoding='latin-1', delimiter=',')\n",
        "test_df  = pd.read_csv(\"/content/test.csv\", encoding='latin-1', delimiter=',')"
      ],
      "metadata": {
        "id": "CLQOKheJAwvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Dataset.from_pandas(train_df)\n",
        "test_ds  = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "3l6dlLZPsdSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "test_ds  = test_ds.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "eLQl62F_shlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"indobenchmark/indobert-base-p1\",\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "Zm5e6y9At9iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_65hxWBXwtj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "11TTXZ8PjzjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = metric.compute(predictions=preds, references=labels)\n",
        "    f1  = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc[\"accuracy\"],\n",
        "        \"macro_f1\": f1[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "_oZD7NSKxi-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training config\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"finetuned-indobert-sentimen\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\"\n",
        ")"
      ],
      "metadata": {
        "id": "fM6v6NxouAUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "MexyLyTyuDxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "3BOCa6ICy5TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"IndoBERT-finetuned-LS\")\n",
        "tokenizer.save_pretrained(\"IndoBERT-finetuned-LS\")"
      ],
      "metadata": {
        "id": "SbAsBdY3y7_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. evaluasi model indobert**"
      ],
      "metadata": {
        "id": "BKQw3PL2mHou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict sentimen menggunakan data test"
      ],
      "metadata": {
        "id": "LjnKwWx6MQgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# 1. Load model IndoBERT yang sudah dilatih\n",
        "model_path = \"IndoBERT-finetuned-LS\"\n",
        "\n",
        "# Load tokenizer from its original pretrained source\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# 2. Fungsi prediksi\n",
        "def predict_sentiment(text):\n",
        "    # tokenisasi\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # prediksi\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # ambil label dengan probabilitas tertinggi\n",
        "    pred_id = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # mapping angka ke label asli\n",
        "    label_map = {0: \"negatif\", 1: \"netral\", 2: \"positif\"}\n",
        "\n",
        "    return label_map[pred_id]"
      ],
      "metadata": {
        "id": "WXocgbhgMQgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"text\"] = test_df[\"text\"].astype(str)"
      ],
      "metadata": {
        "id": "KBAuA3sPMQgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"predicted_label\"] = test_df[\"text\"].apply(predict_sentiment)"
      ],
      "metadata": {
        "id": "2uGwTuFeMQgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {0: \"negatif\", 1: \"netral\", 2: \"positif\"}\n",
        "\n",
        "test_df[\"label\"] = test_df[\"label\"].map(mapping)"
      ],
      "metadata": {
        "id": "7beaihEXMQgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "cDlQN-ipMQgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "bJOM5UCYMQgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluasi dengan confusion matrix menggunakan data test"
      ],
      "metadata": {
        "id": "thA_fUWfMXYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "o8MIN1XYMXYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(test_df[\"label\"],\n",
        "                       test_df[\"predicted_label\"]))"
      ],
      "metadata": {
        "id": "EmuQFoVzMXYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_df[\"label\"],\n",
        "                            test_df[\"predicted_label\"]))"
      ],
      "metadata": {
        "id": "kfBv74bRMXYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. melakukan predict sentimen dengan data selanjutnya**"
      ],
      "metadata": {
        "id": "EpGh3dZwPKA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_belum = pd.read_csv(\"/content/belum_dilabeli.csv\",encoding='latin-1', delimiter=';')"
      ],
      "metadata": {
        "id": "UDCkd7FTm7GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_belum[\"text\"] = df_belum[\"text\"].astype(str)"
      ],
      "metadata": {
        "id": "JD6H_R9PpxFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_belum[\"label\"] = df_belum[\"text\"].apply(predict_sentiment)"
      ],
      "metadata": {
        "id": "FGdv1oh3nHDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_belum.head()"
      ],
      "metadata": {
        "id": "3GZ1GtH9qqlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ambil 80% data yang udh dilabeling otomatis dari indobert buat memperkuat SVM"
      ],
      "metadata": {
        "id": "uMTgToGFs9oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"] = df[\"label\"].map(id2label)\n",
        "df"
      ],
      "metadata": {
        "id": "wdDTF5aSn2fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gabung = pd.concat([df, df_belum])\n",
        "gabung.to_excel(\"data gabungan.xlsx\")"
      ],
      "metadata": {
        "id": "RP1QPnRstJ6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gabung.head()"
      ],
      "metadata": {
        "id": "rYfPSQXZjERN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Feature Engginering (TF - IDF)**"
      ],
      "metadata": {
        "id": "arldkY7Qsv4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "-3TaLcJG78i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dokumen=[]\n",
        "for i in df_bersih['text']:\n",
        "  hasil = i\n",
        "  dokumen.append(i)"
      ],
      "metadata": {
        "id": "KFUuCjeW-WCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b' ,smooth_idf=True, norm=None)\n",
        "X3 = vectorizer.fit_transform(dokumen)\n",
        "df3 = pd.DataFrame(X3.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "df3"
      ],
      "metadata": {
        "id": "gspxF8L2s0Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Membuat model SVM 1** (menggunakan 10% data yang sudah diberi label)"
      ],
      "metadata": {
        "id": "RLYtiv1l75VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stratified split 80:20\n",
        "teks_train_svm_1, teks_test_svm_1, label_train_svm_1, label_test_svm_1 = train_test_split(\n",
        "    df[\"text\"], df[\"label\"],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(teks_train_svm_1))\n",
        "print(\"Test size :\", len(teks_test_svm_1))\n",
        "\n",
        "# gabung lagi jadi dataframe kalau mau\n",
        "train_svm_1 = pd.DataFrame({\"text\": teks_train_svm_1, \"label\": label_train_svm_1})\n",
        "test_svm_1  = pd.DataFrame({\"text\": teks_test_svm_1, \"label\": label_test_svm_1})"
      ],
      "metadata": {
        "id": "r-X3wdeIwbUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pipeline_1 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        lowercase=True,\n",
        "        ngram_range=(1,2),      # sering paling optimal untuk bahasa Indo\n",
        "        max_features=5000\n",
        "    )),\n",
        "    ('svm', LinearSVC())\n",
        "])"
      ],
      "metadata": {
        "id": "HrT6512z7_kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pipeline_1.fit(train_svm_1[\"text\"],train_svm_1[\"label\"])"
      ],
      "metadata": {
        "id": "ACa3Yu1tBWE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = svm_pipeline_1.predict(test_svm_1[\"text\"])\n",
        "\n",
        "hasil_svm = pd.DataFrame({\"teks\":test_svm_1[\"text\"], \"pred\":preds})"
      ],
      "metadata": {
        "id": "dK7nLbiPDYee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_svm_1[\"label\"], preds))"
      ],
      "metadata": {
        "id": "_e2UGE8pk6rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(test_svm_1[\"label\"], preds))"
      ],
      "metadata": {
        "id": "oegiic5jlVad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Membuat model SVM2** (90% data)"
      ],
      "metadata": {
        "id": "pL0wN3wzeLxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stratified split 80:20\n",
        "teks_train, teks_test, label_train, label_test = train_test_split(\n",
        "    gabung[\"text\"], gabung[\"label\"],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=gabung[\"label\"]\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(teks_train))\n",
        "print(\"Test size :\", len(teks_test))\n",
        "\n",
        "# gabung lagi jadi dataframe kalau mau\n",
        "train_svm = pd.DataFrame({\"text\": teks_train, \"label\": label_train})\n",
        "test_svm  = pd.DataFrame({\"text\": teks_test, \"label\": label_test})"
      ],
      "metadata": {
        "id": "SLWl7-KbeLxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        lowercase=True,\n",
        "        ngram_range=(1,2),      # sering paling optimal untuk bahasa Indo\n",
        "        max_features=5000\n",
        "    )),\n",
        "    ('svm', LinearSVC())\n",
        "])"
      ],
      "metadata": {
        "id": "dlibQQG3eLxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pipeline.fit(train_svm[\"text\"],train_svm[\"label\"])"
      ],
      "metadata": {
        "id": "Q23plXqZeLxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = svm_pipeline.predict(test_svm[\"text\"])\n",
        "\n",
        "hasil_svm = pd.DataFrame({\"teks\":test_svm[\"text\"], \"pred\":preds})"
      ],
      "metadata": {
        "id": "R3brY_FYeLxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_svm[\"label\"], preds))"
      ],
      "metadata": {
        "id": "9TfyanwEeLxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(test_svm[\"label\"], preds))"
      ],
      "metadata": {
        "id": "SvClh8lOeLxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. evaluasi kinerja terbaik antara indoBERT vs SVM**"
      ],
      "metadata": {
        "id": "UFIjeJ87jpsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PERBANDINGAN KINERJA MODEL ---\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# IndoBERT metrics\n",
        "bert_acc = accuracy_score(test_df[\"label\"], test_df[\"predicted_label\"])\n",
        "bert_f1  = f1_score(test_df[\"label\"], test_df[\"predicted_label\"], average=\"macro\")\n",
        "\n",
        "# SVM metrics\n",
        "svm_acc = accuracy_score(test_svm[\"label\"], preds)\n",
        "svm_f1  = f1_score(test_svm[\"label\"], preds, average=\"macro\")\n",
        "\n",
        "print(\"=== Perbandingan Model ===\")\n",
        "print(f\"IndoBERT - Accuracy: {bert_acc:.4f}, Macro F1: {bert_f1:.4f}\")\n",
        "print(f\"SVM      - Accuracy: {svm_acc:.4f}, Macro F1: {svm_f1:.4f}\")"
      ],
      "metadata": {
        "id": "yWKv6K52j8iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# IndoBERT\n",
        "bert_acc = accuracy_score(test_df[\"label\"], test_df[\"predicted_label\"])\n",
        "bert_f1  = f1_score(test_df[\"label\"], test_df[\"predicted_label\"], average=\"macro\")\n",
        "\n",
        "# SVM 1 (10% data)\n",
        "preds_svm_1 = svm_pipeline_1.predict(test_svm_1[\"text\"])\n",
        "svm1_acc = accuracy_score(test_svm_1[\"label\"], preds_svm_1)\n",
        "svm1_f1  = f1_score(test_svm_1[\"label\"], preds_svm_1, average=\"macro\")\n",
        "\n",
        "# SVM 2 (90% data)\n",
        "# The 'preds' variable is already correctly calculated for test_svm in a previous cell.\n",
        "svm2_acc = accuracy_score(test_svm[\"label\"], preds)\n",
        "svm2_f1  = f1_score(test_svm[\"label\"], preds, average=\"macro\")\n",
        "\n",
        "print(\"=== Perbandingan Kinerja Model ===\")\n",
        "print(f\"IndoBERT  - Accuracy: {bert_acc:.4f}, Macro F1: {bert_f1:.4f}\")\n",
        "print(f\"SVM 1 (10%) - Accuracy: {svm1_acc:.4f}, Macro F1: {svm1_f1:.4f}\")\n",
        "print(f\"SVM 2 (90%) - Accuracy: {svm2_acc:.4f}, Macro F1: {svm2_f1:.4f}\")"
      ],
      "metadata": {
        "id": "iuUG1UzT5-hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. Topic Modelling LDA**"
      ],
      "metadata": {
        "id": "J5EHvV9BkN06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TOPIC MODELING LDA ---\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Get Indonesian stop words\n",
        "indonesian_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "# gunakan df_bersih atau df_belum\n",
        "lda_text = df_bersih[\"text\"].astype(str)\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_df=0.95,\n",
        "    min_df=5,\n",
        "    stop_words=indonesian_stopwords # Use the loaded list here\n",
        ")\n",
        "\n",
        "X_lda = vectorizer.fit_transform(lda_text)\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=5,\n",
        "    random_state=42,\n",
        "    learning_method='batch'\n",
        ")\n",
        "\n",
        "lda.fit(X_lda)\n",
        "\n",
        "# tampilkan topik\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    print(f\"\\nTOPIK {i+1}\")\n",
        "    print([words[j] for j in topic.argsort()[-15:]])"
      ],
      "metadata": {
        "id": "thGM842ekQyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "id": "9fIQDvbbEory"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = [text.split() for text in gabung[\"text\"]]"
      ],
      "metadata": {
        "id": "9uSXOOBfqpOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "dictionary = Dictionary(tokenized)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized]"
      ],
      "metadata": {
        "id": "CvT1ppCuqtl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=5,\n",
        "    random_state=42,\n",
        "    passes=10\n",
        ")"
      ],
      "metadata": {
        "id": "Dj1GhWW7qzqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "coherence = CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=tokenized,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        ")\n",
        "\n",
        "print(coherence.get_coherence())"
      ],
      "metadata": {
        "id": "YHGwb9P1q02l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "coherence = CoherenceModel(model=lda, texts=tokenized, dictionary=dictionary, coherence='c_v')\n",
        "print(coherence.get_coherence())"
      ],
      "metadata": {
        "id": "_FJF1tWRX_Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. Visualisasi (Wordcloud + Grafik Topik)**"
      ],
      "metadata": {
        "id": "34GWwiBjlONx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  WORDCLOUD UNTUK SETIAP TOPIK\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    wc = WordCloud(background_color=\"white\")\n",
        "    wc.generate_from_frequencies(dict(zip(words, topic)))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Wordcloud Topik {i+1}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XJeBAf5zlTUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Kesimpulan**"
      ],
      "metadata": {
        "id": "yyzmvLlhla62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== KESIMPULAN PROYEK ===\")\n",
        "\n",
        "if bert_f1 > svm_f1:\n",
        "    best = \"IndoBERT\"\n",
        "else:\n",
        "    best = \"SVM\"\n",
        "\n",
        "print(f\"Model terbaik berdasarkan Macro F1: {best}\")\n",
        "\n",
        "print(\"\\nTopik utama yang muncul dari komentar:\")\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    top_words = [words[j] for j in topic.argsort()[-8:]]\n",
        "    print(f\"- Topik {i+1}: {', '.join(top_words)}\")\n"
      ],
      "metadata": {
        "id": "y7NBDuk5lckt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}